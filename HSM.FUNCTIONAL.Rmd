HSM NOTEBOOK 
MEMOIRE - PLASMAN CHARLIE
2023

```{r COLOR PALETTES}
library(RColorBrewer)
my.palette.oranges = brewer.pal(n = 9, name = "Oranges") 
my.palette.blue = rev(brewer.pal(n = 9, name = "Blues"))
palettemulticolor = colorRampPalette(c("deepskyblue", "darkseagreen","lightgreen", "green","yellow","gold","orange", "red","firebrick"))(100)
bluepalette=colorRampPalette(c("blue4","blue","dodgerblue", "deepskyblue", "lightskyblue"))(800)
```

```{r ENVIRONNEMENTAL DESCRIPTORS}
library(raster)
library(ncdf4)
setwd("/Users/charlieplasman/Desktop/Mémoire /HSM/Personal data/Environmental data/Benthic layers")
# List all rasters from a dir and load them in a stack
files_names = list.files("/Users/charlieplasman/Desktop/Mémoire /HSM/Personal data/Environmental data/Benthic layers",pattern = ".nc$")
extent <- extent(-180, 180, -80, -50)
raster_list = list()
for (i in seq_along(files_names)) {
  raster_list[[i]] = raster(files_names[i])
  raster_list[[i]] <- crop(raster_list[[i]], extent)
}
predictors_stack = stack(raster_list)
plot(subset(predictors_stack,6), col=bluepalette, cex=0.8, legend.width=0.5, legend.shrink=0.4,legend.args=list(text="Depth (m)", side=3, font=2, cex=0.8))

```

```{r BASEMAP}
library(SOmap)
library(SDMPlay)
basemap = SOmap(bathy_legend = T, graticules = T, fronts = T, border_width = 0.8, trim = -50, ice = T) 
plot(basemap)
```

```{r OCCURRENCE DATA}
setwd("/Users/charlieplasman/Desktop/Mémoire /HSM/Personal data/Species data/Preys/Automatic")
library(readr)
occ = read.csv("Coords_Euphausiidae.csv", sep = ",")
# Extract longitude and latitude values
spat_occ = cbind.data.frame(occ$longitude,occ$latitude)
colnames(spat_occ) = c("longitude", "latitude")
head(spat_occ)
```
```{r GET LAND DATA AND FILTER POINTS}

library(rnaturalearth)
# Get the countries dataset from rnaturalearth
countries <- ne_countries(scale = "medium", returnclass = "sf")
# Subset the dataset to include only Antarctica
antarctica <- subset(countries, continent == "Antarctica")
# Create a raster layer for Antarctica using the extent of the subsetted polygon
raster_antarctica <- raster(extent(antarctica), resolution = 0.1)
# Set all cells in the raster to NA
raster_antarctica[] <- NA
# Create a mask using the Antarctica polygon
mask <- rasterize(antarctica, raster_antarctica)
# Set the cells within the mask to a value of 1
raster_antarctica[mask == 1] <- 1
# Plot the raster
plot(raster_antarctica, col = "lightblue", legend = F)

plot(basemap)
SOplot(spat_occ)
# Load land mask data
land_mask <- raster_antarctica
# Create a logical vector indicating if each point is at sea
on_land <- !is.na(extract(land_mask, spat_occ))
# Subset the spatial point data to keep only points at sea
spat_occ_land <- spat_occ[on_land, ]
library(dplyr)
# Remove points on land from spat_occ
spat_occ_sea <- anti_join(spat_occ, spat_occ_land)
spat_occ = spat_occ_sea
# Plot the remaining points on the basemap
plot(basemap)
SOplot(spat_occ_sea)

```

```{r STUDY EXTENT}

extent = extent(-180, 180, -80, -50)
# Filter the data frame based on the extent
spat_occ = spat_occ[spat_occ$longitude >= extent@xmin & spat_occ$longitude <= extent@xmax & spat_occ$latitude >= extent@ymin & spat_occ$latitude <= extent@ymax, ]
plot(basemap)
SOplot(spat_occ)

```

```{r ENVIRONNMENTAL COLLINEARITY ANALYSIS}
library(usdm)
collinear_var = vifstep(predictors_stack)
raster_names = names(predictors_stack)
for (var in collinear_var@excluded) {
  raster_names = raster_names[!grepl(var, raster_names)]
}
#raster_names <- raster_names[!(raster_names %in% c("ice_thickness_min", "ice_cover_min","ice_cover_mean","ice_cover_range"))]
# Keep only rasters that are not collinear
stack_linear = predictors_stack[[raster_names]]
```

```{r KERNEL DENSITY ESTIMATION}

KDE_layer = raster(MASS::kde2d(spat_occ$longitude, spat_occ$latitude,
                                n=c(ncol(stack_linear), nrow(stack_linear)),
                                lims=c( -180, 180, -80, -50))) 
extent(KDE_layer) = extent(stack_linear)
KDE_layer = mask(KDE_layer, subset(stack_linear,1)) 
plot(basemap)
SOplot(KDE_layer) 
HSMtable_KDE= SDMPlay:::SDMtab(xydata=spat_occ, predictors=stack_linear, unique.data=TRUE,same=TRUE, KDE=KDE_layer)
background_detail_KDE = subset(HSMtable_KDE, HSMtable_KDE$id==0)[,c(2,3)] 

```

```{r KDE PROCEDURE}

# Plot the background points from the random and KDE procedure
plot(basemap)
SOplot(stack_linear$depth, col=bluepalette)
SOplot(background_detail_KDE, pch=20, col= "orange")

```

```{r POINTS FILTRATION (NA VALUES -> NTERPOLATION ?)}
# Assess the proportion of points with NA value
predictors_qual = SDMPlay:::SDMdata.quality(data = HSMtable_KDE)
print(predictors_qual)

threshold = 30
selected_predictors = names(HSMtable_KDE)
for (i in 1:nrow(predictors_qual)) {
  if (predictors_qual[i,1] >= threshold) {
    selected_predictors = setdiff(selected_predictors, names(HSMtable_KDE)[i+3])
  }
}

HSMtable_KDE = HSMtable_KDE[(selected_predictors)]
stack_linear_clean = stack_linear[[selected_predictors]] 

```

```{r INTERPOLATION}
library(imputeTS)
library(dplyr)

names = names(stack_linear_clean)

# Fonction pour l'interpolation linéaire des variables
interpolate_variables <- function(data) {
  interpolated_data <- data %>%
    group_by(longitude, latitude) %>%
    mutate(across(names, ~na_interpolation(., option = "linear")))
  
  return(interpolated_data)
}

# Appel de la fonction pour interpoler les variables
interpolated_data <- interpolate_variables(data)

# Affichage des données interpolées
print(interpolated_data)

```

```{r SPATIAL CROSS-VALIDATION (CLOCK 6) - NO LOOP}
setwd("/Users/charlieplasman/Desktop/Mémoire /HSM/Personal data")
# Choose the occurrence data (random/KDE)
data = HSMtable_KDE
envi_data = stack_linear

# Create replicates with different partitioning
idP = which(data$id == 1) 
partition_function = SDMPlay:::clock6(data[idP, c("longitude", "latitude")],
                                         data[-idP, c("longitude", "latitude")])
MyFold = rep(NA, nrow(data))
MyFold[idP]= partition_function$occ.grp 
MyFold[-idP]= partition_function$bg.coords.grp
{
  plot(basemap)
  SOplot(data[, c("longitude", "latitude")],
         col = c("orange", "darkblue", "darkblue", "darkblue", "darkblue", "darkblue")[as.factor(MyFold)],
         pch = 20)
  SOleg(col = c("orange", "darkblue"), position = "topright",
        tlabs = c("training", "test"), type = "discrete")
}
model_output = SDMPlay:::compute.brt(x = data, proj.predictors = envi_data,
                                        tc = 5, lr = 0.005, bf = 0.75, n.trees = 500, step.size = 100,
                                        n.folds = 6,
                                        fold.vector = MyFold)
plot(basemap)
SOplot(model_output$raster.prediction, col = my.palette.oranges)

```
```{r}
library(SDMPlay)

setwd("/Users/charlieplasman/Desktop/Mémoire /HSM/Personal data")
# Choose the occurrence data (random/KDE)
data <- HSMtable_KDE
envi_data <- stack_linear

n_repeats <- 6  # Number of repetitions
model_outputs <- vector("list", n_repeats)  # List to store the model outputs

for (i in 1:n_repeats) {
  # Create replicates with different partitioning
  idP <- which(data$id == 1)
  partition_function <- SDMPlay:::clock6(data[idP, c("longitude", "latitude")], data[-idP, c("longitude", "latitude")])
  MyFold <- rep(NA, nrow(data))
  MyFold[idP] <- partition_function$occ.grp
  MyFold[-idP] <- partition_function$bg.coords.grp
  
  {
    plot(basemap)
    SOplot(data[, c("longitude", "latitude")],
           col = c("orange", "darkblue", "darkblue", "darkblue", "darkblue", "darkblue")[as.factor(MyFold)],
           pch = 20)
    SOleg(col = c("orange", "darkblue"), position = "topright",
          tlabs = c("training", "test"), type = "discrete")
  }
  
  model_output <- SDMPlay:::compute.brt(x = data, proj.predictors = envi_data,
                              tc = 5, lr = 0.005, bf = 0.75, n.trees = 500, step.size = 100,
                              n.folds = 6,
                              fold.vector = MyFold)
  model_outputs[[i]] <- model_output
}

# Initialize an empty list to store the raster predictions
raster_predictions <- vector("list", length(model_outputs))

# Extract raster predictions from each model output
for (i in 1:length(model_outputs)) {
  raster_predictions[[i]] <- model_outputs[[i]]$raster.prediction
}

# Create the stacked raster
stack.pred <- stack(raster_predictions[1:3])

# Calculate the average prediction
average_pred <- calc(stack.pred, mean)

plot(basemap)
SOplot(average_pred, col = my.palette.oranges)
```


```{r SAVE}
dir = "/Users/charlieplasman/Desktop/Mémoire /HSM/Personal data/Species data/Preys/Automatic"
writeRaster(average_pred, filename=file.path(dir, "Nototheniidae.nc"), format="CDF", overwrite=TRUE)
setwd("/Users/charlieplasman/Desktop/Mémoire /HSM/Personal data/Species data/Preys/Automatic")
prey = raster("Nototheniidae.nc")
plot(prey)
tresh = 0.5
prey[prey < tresh] <- NA
plot(prey)
```

```{r}
library(SDMPlay)

setwd("/Users/charlieplasman/Desktop/Mémoire /HSM/Personal data")
# Choose the occurrence data (random/KDE)
data <- HSMtable_KDE
envi_data <- stack_linear

n_repeats <- 10  # Number of repetitions
model_outputs <- vector("list", n_repeats)  # List to store the model outputs

for (i in 1:n_repeats) {
  model_output <- SDMPlay:::compute.brt(x = data, proj.predictors = envi_data,
                              tc = 5, lr = 0.005, bf = 0.75, n.trees = 500, step.size = 100)
  model_outputs[[i]] <- model_output
}

# Initialize an empty list to store the raster predictions
raster_predictions <- vector("list", length(model_outputs))

# Extract raster predictions from each model output
for (i in 1:length(model_outputs)) {
  raster_predictions[[i]] <- model_outputs[[i]]$raster.prediction
}

# Create the stacked raster
stack.pred <- stack(raster_predictions[1:10])

# Calculate the average prediction
average_pred <- calc(stack.pred, mean)

plot(basemap)
SOplot(average_pred)

```

```{r}
stack.pred <- stack(model_output_1$raster.prediction,model_output_2$raster.prediction,model_output_3$raster.prediction)
average_pred <- calc(stack.pred, mean)
```

```{r PLOT OUTPUTS}
par(mfrow = c(1, 1))
plot(basemap)
SOplot(model_output_3$raster.prediction, col = my.palette.oranges)  

plot(basemap)
SOplot(average_pred, add = T, col = my.palette.oranges) 
```

```{r CONTRIBUTION AND INTERACTION DESCRIPTORS}

model_output = model_output_3
library(dismo)
# Calculate contribution of descriptors
contributions = model_output$response$contributions
b <- barplot(contributions[, 2], ylab = "Contribution (%)", names.arg = contributions[,1])
contributions
#Calculate interactions between descriptors
gbm.plot(model_output$response,n.plots=21,cex.axis=0.6,cex.lab=0.7, smooth=TRUE)
interactions = gbm.interactions(model_output$response) 
head(interactions$rank.list[,c(5,2,4)])
gbm.perspec(model_output$response,interactions$rank.list[1,1], interactions$rank.list[1,3], cex.lab=0.6, cex.axis=0.6)
```

```{r SUITABLE AREA}
# Binarize model predictions
maxSSS = model_output$eval.stats$maxSSS
{
plot(basemap)
SOplot(model_output$raster.prediction, col=c("lightblue","red"), breaks=c(0, maxSSS ,1), main="Prediction for cluster 1",
     cex.axis= 0.7,
     legend.width=0.5, legend.shrink=0.25,
     legend.args=list(text="Distribution probability", side=3, font=2, cex=0.7)) 
}
# Extract predictions at test data location
location_presence_test = data[as.factor(MyFold)==1,c(2,3)] 
extracted_values = raster::extract(model_output$raster.prediction,location_presence_test) #extracted_values
# Compare the values with the maxSSS value and evaluate the percentage of correctly classified presence test data
100* length (which(na.omit(extracted_values) >= maxSSS)) / length(na.omit(extracted_values))
```

```{r MODEL STATISTICS}
model_output$eval.stats$AUC #AUC
model_output$eval.stats$TSS #TSS
model_output$eval.stats$COR #COR
```

```{r EXTRAPOLATION - MESS}
# Extract environmental values at presence-only records location
envi_scores = raster::extract(envi_data, data[,2:3]) 
# Calculate MESS score
x = dismo::mess(envi_data,envi_scores)
y = x; values(y)= values(x)>0
y = reclassify(y,cbind(FALSE,0)) # MODEL EXTRAPOLATES (MESS <0)
y = reclassify(y,cbind(TRUE,1)) # MODEL DOES NOT EXTRAPOLATE (MESS >0)
MESS_layer = mask(y,subset(envi_data,1)) # remove land pixel (layer #1 = depth)
{
plot(basemap)
SOplot (MESS_layer, col=c("grey","blue"), legend=F) 
legend("bottomleft", legend=c("extrapolation","OK"),
       col=c("grey","blue"),
       pch=20, bg = "white", cex=0.8)
}
# Calculate the proportion of the area where extrapolation occurs
MESS= reclassify(MESS_layer,cbind(1,NA))
# compare the number of pixels = 0 to the number of total pixels of the area
length(which(!is.na(values(MESS))))*100 / length(which(!is.na(values(subset(predictors_stack,1)))))
# Assess which environmental descriptors are responsible for extrapolation at each pixel
```

```{r EXTRAPOLATION - MESS SPLIT}
# Create an empty raster to initiate a Rasterstack
stack_amelio_MESS = subset(predictors_stack,1); values(stack_amelio_MESS) = NA # Loop to calculate the value of dissimilarity of each environmental descriptor
# For each pixel, it will be determined if extrapolation occurs for each environmental descriptor
for (k in 1:nlayers(envi_data)){
  presvals = raster::extract(subset(envi_data, k),
                              data[,2:3])
  x_amelio = dismo::mess(subset(envi_data, k),presvals)
  stack_amelio_MESS = stack(stack_amelio_MESS,x_amelio) 
}
# Delete the first layer of the stack that was empty (initialization)
stack_amelio_MESS = dropLayer(stack_amelio_MESS,1) 
names(stack_amelio_MESS) = names(envi_data)
# Search for the environmental layer that is responsible for the lower MESS score
# (i.e. responsible for the extrapolation)
MESS_amelio = which.min(stack_amelio_MESS)
MESS_amelio = mask(MESS_amelio, MESS) # keep only areas where extrapolation occurs
# Plot the result
{
plot(basemap)
SOplot (MESS_amelio, col=palettemulticolor, legend=F) 
points(worldmap, type="l")
legend("bottomleft", legend=names(predictors_stack),
       col=palettemulticolor, pch=20, bg = "white", cex=0.5)
}
```

```{r EXTRAPOLATION - CONTRIBUTIONS}
# Calculate the contribution of each environmental descriptor in extrapolation
table_mess_amelio = matrix(data=NA, nrow = 1, ncol= nlayers(envi_data)) 
colnames(table_mess_amelio) =names(envi_data)
for (k2 in 1:nlayers(envi_data)){
  table_mess_amelio[1,k2] = length(which(values(MESS_amelio)==k2))*100/
    length(which(!is.na(values(subset(envi_data,1))))) }
table = data.frame(table_mess_amelio)
```

```{r SPATIAL AUTOCORRELATION}
library(ape)
xy <- data[, 2:3]
dists <- as.matrix(dist(xy))
dists.inv <- 1/dists
diag(dists.inv) <- 0
dists.inv[is.infinite(dists.inv)] <- max(dists[!is.infinite(dists)])
res <- c(model_output$eval.stats$residuals[[1]])
Moran.I(res, dists.inv)
```

```{r CORRELATION}
library(ncf)
rsd = res
rnd = sample(1:length(rsd), 500, replace=T)
spat.cor = correlog(xy[rnd,1], xy[rnd,2], rsd[rnd], increment = 2, resamp = 10)
par(mfcol =c(1,2))
{
plot(spat.cor$mean.of.class, spat.cor$correlation, ylim = c(-.005,.005),xlim = c(0,50), pch=16, col="firebrick3", ylab ="Correlation",xlab="Distance class", main="Spatial Correlogram", font.lab=2)
lines(spat.cor$mean.of.class, spat.cor$correlation, col = "firebrick3")
abline(0,0,col="grey50",lty=3)
plot(xy[order(rsd),],pch=15, col = "firebrick3" ,cex=.3, main="Residuals",xlab="Latitude",ylab="Longitude", font.lab=2)
par(mfcol=c(1,1))
}
```

```{r BRT PARAMETERS}
library(gbm)
model1 = dismo::gbm.step(data = data, gbm.x = 4:ncol(data), gbm.y = 1, tree.complexity = 2, learning.rate = 0.005,
                          bag.fraction = 0.75, n.trees = 500, step.size = 100)
model2 = dismo::gbm.step(data = data, gbm.x = 4:ncol(data), gbm.y = 1, tree.complexity = 2, learning.rate = 0.001,
                          bag.fraction = 0.75, n.trees = 500, step.size = 100)
model3 = dismo::gbm.step(data = data, gbm.x = 4:ncol(data), gbm.y = 1, tree.complexity = 2, learning.rate = 0.0005,
                          bag.fraction = 0.75, n.trees = 500, step.size = 100)
model4 = dismo::gbm.step(data = data, gbm.x = 4:ncol(data), gbm.y = 1, tree.complexity = 2, learning.rate = 0.0001,
                          bag.fraction = 0.75, n.trees = 500, step.size = 100)
tree.list1 = seq(100, model1$gbm.call$best.trees, by = 100)
tree.list2 = seq(100, model2$gbm.call$best.trees, by = 100)
tree.list3 = seq(100, model3$gbm.call$best.trees, by = 100)
#tree.list4 = seq(100, model4$gbm.call$best.trees, by = 100)
pred1 = predict.gbm(model1,data, n.trees = tree.list1, "response") 
pred2 = predict.gbm(model2,data, n.trees = tree.list2, "response") 
pred3 = predict.gbm(model3,data, n.trees = tree.list3, "response") 
#pred4 = predict.gbm(model4,data, n.trees = tree.list4, "response")
# comparison of predictions with observed values : measure of predicted deviance
library(dismo)
graphe.deviance1 = rep(0,max(tree.list1)/100) 
for (i in 1:length(graphe.deviance1)) {
  graphe.deviance1 [i] = calc.deviance(data$id, pred1[,i],calc.mean=T) }
graphe.deviance2 = rep(0,max(tree.list2)/100) 
for (i in 1:length(graphe.deviance2)) {
  graphe.deviance2 [i] = calc.deviance(data$id, pred2[,i],calc.mean=T) }
graphe.deviance3 = rep(0,max(tree.list3)/100) 
for (i in 1:length(graphe.deviance3)) {
  graphe.deviance3 [i] = calc.deviance(data$id, pred3[,i],calc.mean=T) }
graphe.deviance4 = rep(0,max(tree.list4)/100) 
#for (i in 1:length(graphe.deviance4)) {
  #graphe.deviance4 [i] = calc.deviance(data$id, pred4[,i],calc.mean=T) }
par(mfrow = c(1, 1))
plot(tree.list1,graphe.deviance1,xlim = c(-100,10000), ylim=c(0,1.5),type="l", xlab = "Number of trees",
     ylab = "Predictive deviance", cex.lab = 1.5, col="black")
lines(tree.list2,graphe.deviance2,col="blue") 
lines(tree.list3,graphe.deviance3,col="red") 
#lines(tree.list4,graphe.deviance4,col="green",xlab = "Number of trees", ylab = "Predictive deviance")
legend("bottomright",legend=c("tc5 lr0.005 bf=0.5","tc5 lr0.005 bf=0.75","tc5 lr0.005 bf=1"), col=c("black","blue","red"),pch=16,cex=1)
```


```{r BRT PARAMETERS}

library(gbm)
library(dismo)

learning_rates <- c(0.01, 0.005, 0,001)
tree_complexities <- c(1, 2, 3, 4, 5)
bag_fractions <- c(0.5, 0.7, 0.8)

results <- matrix(NA, nrow = length(learning_rates), ncol = length(tree_complexities) * length(bag_fractions))

for (i in seq_along(learning_rates)) {
  for (j in seq_along(tree_complexities)) {
    for (k in seq_along(bag_fractions)) {
      model <- gbm.step(data = data, gbm.x = 4:ncol(data), gbm.y = 1,
                        tree.complexity = tree_complexities[j],
                        learning.rate = learning_rates[i],
                        bag.fraction = bag_fractions[k],
                        n.trees = 500, step.size = 500)
      tree_list <- seq(100, model$n.trees, by = 100)
      pred <- predict.gbm(model, data, n.trees = tree_list, "response")
      deviance <- rep(0, length(tree_list)/100)
      for (m in 1:length(deviance)) {
        deviance[m] <- calc.deviance(data$id, pred[, m], calc.mean = TRUE)
      }
      results[i, (j - 1) * length(bag_fractions) + k] <- max(deviance)
    }
  }
}

# Plot the results
colors <- c("black", "blue", "red", "green", "purple")
legend_labels <- expand.grid(learning_rates, tree_complexities, bag_fractions) %>%
  mutate(label = paste0("lr", Var1, " tc", Var2, " bf", Var3))

plot(1, type = "n", xlim = c(-100, 10000), ylim = c(0, 1.5),
     xlab = "Number of trees", ylab = "Predictive deviance", cex.lab = 1.5)
for (i in seq_along(legend_labels$label)) {
  lines(tree_list, results[i, ], col = colors[i])
}
legend("bottomright", legend = legend_labels$label, col = colors, pch = 16, cex = 1)

```

